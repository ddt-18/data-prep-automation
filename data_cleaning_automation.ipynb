{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8c0ed5d-a9da-4416-94d4-b383b8c8fc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <ins><a href=\"https://ydata.ai/register\">Upgrade to ydata-sdk</a></ins>\n",
       "                <p>\n",
       "                    Improve your data and profiling with ydata-sdk, featuring data quality scoring, redundancy detection, outlier identification, text validation, and synthetic data generation.\n",
       "                </p>\n",
       "            </div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter dataset folder path (e.g., C:\\Users\\Admin\\Documents\\Datasets) [default: current directory]:  C:\\Users\\Admin\\Documents\\Automation\\Datasets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available datasets:\n",
      "  1. ncr_ride_bookings.csv\n",
      "  2. SaaS_Customer_Retention_Data.xlsx\n",
      "  3. sales_data_sample.csv\n",
      "  4. training.1600000.processed.noemoticon.csv\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter number(s) of dataset(s) to process, comma-separated (e.g., 1,3) [default: last]:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Processing ncr_ride_bookings ====================\n",
      "\n",
      "‚è≥ Loading data from: ncr_ride_bookings.csv\n",
      "‚úÖ Loaded successfully with shape (150000, 21)\n",
      "         Date      Time    Booking ID   Booking Status   Customer ID  \\\n",
      "0  2024-03-23  12:29:38  \"CNR5884300\"  No Driver Found  \"CID1982111\"   \n",
      "1  2024-11-29  18:01:39  \"CNR1326809\"       Incomplete  \"CID4604802\"   \n",
      "2  2024-08-23  08:56:10  \"CNR8494506\"        Completed  \"CID9202816\"   \n",
      "3  2024-10-21  17:17:25  \"CNR8906825\"        Completed  \"CID2610914\"   \n",
      "4  2024-09-16  22:08:00  \"CNR1950162\"        Completed  \"CID9933542\"   \n",
      "\n",
      "    Vehicle Type      Pickup Location      Drop Location  Avg VTAT  Avg CTAT  \\\n",
      "0          eBike          Palam Vihar            Jhilmil       NaN       NaN   \n",
      "1       Go Sedan        Shastri Nagar  Gurgaon Sector 56       4.9      14.0   \n",
      "2           Auto              Khandsa      Malviya Nagar      13.4      25.8   \n",
      "3  Premier Sedan  Central Secretariat           Inderlok      13.1      28.5   \n",
      "4           Bike     Ghitorni Village        Khan Market       5.3      19.6   \n",
      "\n",
      "   ...  Reason for cancelling by Customer Cancelled Rides by Driver  \\\n",
      "0  ...                                NaN                       NaN   \n",
      "1  ...                                NaN                       NaN   \n",
      "2  ...                                NaN                       NaN   \n",
      "3  ...                                NaN                       NaN   \n",
      "4  ...                                NaN                       NaN   \n",
      "\n",
      "   Driver Cancellation Reason Incomplete Rides  Incomplete Rides Reason  \\\n",
      "0                         NaN              NaN                      NaN   \n",
      "1                         NaN              1.0        Vehicle Breakdown   \n",
      "2                         NaN              NaN                      NaN   \n",
      "3                         NaN              NaN                      NaN   \n",
      "4                         NaN              NaN                      NaN   \n",
      "\n",
      "  Booking Value  Ride Distance  Driver Ratings  Customer Rating  \\\n",
      "0           NaN            NaN             NaN              NaN   \n",
      "1         237.0           5.73             NaN              NaN   \n",
      "2         627.0          13.58             4.9              4.9   \n",
      "3         416.0          34.02             4.6              5.0   \n",
      "4         737.0          48.21             4.1              4.3   \n",
      "\n",
      "   Payment Method  \n",
      "0             NaN  \n",
      "1             UPI  \n",
      "2      Debit Card  \n",
      "3             UPI  \n",
      "4             UPI  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "‚è≥ Cleaning data...\n",
      "‚úÖ Cleaning complete. Shape: (150000, 21)\n",
      "\n",
      "--- Data Cleaning Options ---\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Handle missing values?\n",
      " [1] Drop rows\n",
      " [2] Fill mean/mode\n",
      " [3] Do nothing\n",
      " Choice:  2\n",
      "Drop duplicate rows? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Visualization Options ---\n",
      "Available columns: Index(['Date', 'Time', 'Booking_ID', 'Booking_Status', 'Customer_ID',\n",
      "       'Vehicle_Type', 'Pickup_Location', 'Drop_Location', 'Avg_VTAT',\n",
      "       'Avg_CTAT', 'Cancelled_Rides_by_Customer',\n",
      "       'Reason_for_cancelling_by_Customer', 'Cancelled_Rides_by_Driver',\n",
      "       'Driver_Cancellation_Reason', 'Incomplete_Rides',\n",
      "       'Incomplete_Rides_Reason', 'Booking_Value', 'Ride_Distance',\n",
      "       'Driver_Ratings', 'Customer_Rating', 'Payment_Method'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Manually select columns for visualizations? (y/n):  n\n",
      "Enter sample size for visualizations (e.g., 50000) [default: full dataset]:  none\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Cleaning data...\n",
      "  - Dropped 0 duplicate rows.\n",
      "  - Filled missing values with mean/mode.\n",
      "‚úÖ Cleaning complete. Shape: (150000, 21)\n",
      "üíæ Saving cleaned dataset...\n",
      "‚úÖ Saved CSV -> output\\ncr_ride_bookings_cleaned_20250902_020728.csv\n",
      "‚úÖ Saved Excel -> output\\ncr_ride_bookings_cleaned_20250902_020728.xlsx\n",
      "üìä Creating visualizations...\n",
      "  - Bar chart: Booking_Status\n",
      "  - Bar chart: Vehicle_Type\n",
      "  - Pie chart: Booking_Status\n",
      "  - Pie chart: Vehicle_Type\n",
      "‚úÖ Plots saved in output\\ncr_ride_bookings_plots\n",
      "üìë Building profiling report...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Report saved -> output\\ncr_ride_bookings_report_20250902.html\n",
      "\n",
      "üéâ Automation complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from ydata_profiling import ProfileReport\n",
    "import warnings\n",
    "from io import StringIO\n",
    "from contextlib import redirect_stdout\n",
    "import logging\n",
    "from multiprocessing import Pool\n",
    "import tqdm  # Import tqdm to disable progress bars\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Disable tqdm progress bars globally\n",
    "tqdm.tqdm().disable = True\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIGURATION\n",
    "# -----------------------------\n",
    "\n",
    "# Dynamic data folder input with validation\n",
    "def get_valid_directory():\n",
    "    \"\"\"Prompts for and validates a directory path.\"\"\"\n",
    "    while True:\n",
    "        data_folder = input(\"Enter dataset folder path (e.g., C:\\\\Users\\\\Admin\\\\Documents\\\\Datasets) [default: current directory]: \").strip() or os.getcwd()\n",
    "        if os.path.isfile(data_folder):\n",
    "            logging.warning(f\"Provided path is a file, not a directory: {data_folder}\")\n",
    "            print(f\"‚ö†Ô∏è Error: '{data_folder}' is a file, not a directory. Using parent directory.\")\n",
    "            data_folder = os.path.dirname(data_folder)\n",
    "        if os.path.isdir(data_folder):\n",
    "            return data_folder\n",
    "        else:\n",
    "            logging.error(f\"Invalid directory: {data_folder}\")\n",
    "            print(f\"‚ùå Error: '{data_folder}' is not a valid directory. Please try again.\")\n",
    "\n",
    "DATA_FOLDER = get_valid_directory()\n",
    "OUTPUT_FOLDER = \"output\"\n",
    "SHOW_PREVIEW = True\n",
    "EXCLUDE_PATTERNS = ['id', 'desc', 'number', 'phone', 'contact', 'name']\n",
    "VISUALIZATION_OPTIONS = {\n",
    "    'figure_size': (10, 6),  # Hardcoded default figure size\n",
    "    'colors': ['#FF6384', '#36A2EB', '#FFCE56', '#4BC0C0', '#9966FF']\n",
    "}\n",
    "\n",
    "# Setup logging\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    filename=os.path.join(OUTPUT_FOLDER, f'processing_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'),\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def list_and_select_files(data_folder: str) -> list:\n",
    "    \"\"\"Lists available datasets and lets user pick one or more for processing.\"\"\"\n",
    "    try:\n",
    "        files = [f for f in os.listdir(data_folder) if f.lower().endswith((\".xlsx\", \".xls\", \".csv\", \".json\", \".parquet\"))]\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"The specified data folder does not exist: {data_folder}\")\n",
    "        print(f\"‚ùå ERROR: The specified data folder does not exist: {data_folder}\")\n",
    "        return []\n",
    "    except OSError as e:\n",
    "        logging.error(f\"Error accessing directory {data_folder}: {e}\")\n",
    "        print(f\"‚ùå ERROR: Unable to access directory {data_folder}: {e}\")\n",
    "        return []\n",
    "\n",
    "    if not files:\n",
    "        logging.warning(f\"No datasets found in: {data_folder}\")\n",
    "        print(f\"‚ùå No datasets found in: {data_folder}\")\n",
    "        return []\n",
    "\n",
    "    print(\"\\nAvailable datasets:\")\n",
    "    for i, f in enumerate(files, 1):\n",
    "        print(f\"  {i}. {f}\")\n",
    "\n",
    "    choice_str = input(\"\\nEnter number(s) of dataset(s) to process, comma-separated (e.g., 1,3) [default: last]: \").strip()\n",
    "\n",
    "    selected_files = []\n",
    "    if not choice_str:\n",
    "        selected_files.append(os.path.join(data_folder, files[-1]))  # Default to last file\n",
    "    else:\n",
    "        try:\n",
    "            choices = [int(c.strip()) - 1 for c in choice_str.split(\",\")]\n",
    "            selected_files = [os.path.join(data_folder, files[i]) for i in choices if 0 <= i < len(files)]\n",
    "            if not selected_files:\n",
    "                logging.warning(\"No valid file indices selected. Defaulting to the last dataset.\")\n",
    "                print(\"‚ö†Ô∏è No valid file indices selected. Defaulting to the last dataset.\")\n",
    "                selected_files.append(os.path.join(data_folder, files[-1]))\n",
    "        except (ValueError, IndexError):\n",
    "            logging.error(f\"Invalid input: {choice_str}. Defaulting to the last dataset.\")\n",
    "            print(\"‚ö†Ô∏è Invalid input. Defaulting to the last dataset.\")\n",
    "            selected_files.append(os.path.join(data_folder, files[-1]))\n",
    "\n",
    "    return selected_files\n",
    "\n",
    "def load_data(file_path: str) -> pd.DataFrame | None:\n",
    "    \"\"\"Loads dataset (CSV, Excel, JSON, Parquet) into a pandas DataFrame.\"\"\"\n",
    "    print(f\"\\n‚è≥ Loading data from: {os.path.basename(file_path)}\")\n",
    "    logging.info(f\"Loading data from: {file_path}\")\n",
    "    try:\n",
    "        if file_path.lower().endswith(\".csv\"):\n",
    "            encodings = ['utf-8', 'latin1', 'iso-8859-1']\n",
    "            for enc in encodings:\n",
    "                try:\n",
    "                    df = pd.read_csv(file_path, encoding=enc)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Tried encoding {enc} for {file_path}: {e}\")\n",
    "            else:\n",
    "                raise ValueError(\"Failed to load CSV with any encoding\")\n",
    "        elif file_path.lower().endswith((\".xlsx\", \".xls\")):\n",
    "            df = pd.read_excel(file_path)\n",
    "        elif file_path.lower().endswith(\".json\"):\n",
    "            df = pd.read_json(file_path)\n",
    "        elif file_path.lower().endswith(\".parquet\"):\n",
    "            df = pd.read_parquet(file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format\")\n",
    "        \n",
    "        print(f\"‚úÖ Loaded successfully with shape {df.shape}\")\n",
    "        logging.info(f\"Loaded successfully: {file_path}, shape {df.shape}\")\n",
    "        if SHOW_PREVIEW:\n",
    "            print(df.head())\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading {os.path.basename(file_path)}: {e}\")\n",
    "        print(f\"‚ùå Error loading {os.path.basename(file_path)}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_cleaning_choices(df_columns: list, row_count: int) -> dict:\n",
    "    \"\"\"Asks the user for cleaning, visualization, and sampling preferences with validation.\"\"\"\n",
    "    print(\"\\n--- Data Cleaning Options ---\")\n",
    "    \n",
    "    while True:\n",
    "        mv_choice = input(\"Handle missing values?\\n [1] Drop rows\\n [2] Fill mean/mode\\n [3] Do nothing\\n Choice: \").strip()\n",
    "        if mv_choice in ['1', '2', '3']:\n",
    "            break\n",
    "        print(\"‚ö†Ô∏è Invalid choice. Please select 1, 2, or 3.\")\n",
    "\n",
    "    while True:\n",
    "        dd_choice = input(\"Drop duplicate rows? (y/n): \").strip().lower()\n",
    "        if dd_choice in ['y', 'n']:\n",
    "            break\n",
    "        print(\"‚ö†Ô∏è Invalid choice. Please select y or n.\")\n",
    "\n",
    "    bar_cols, pie_cols = [], []\n",
    "    print(\"\\n--- Visualization Options ---\")\n",
    "    print(f\"Available columns: {df_columns}\")\n",
    "    if input(\"Manually select columns for visualizations? (y/n): \").strip().lower() == 'y':\n",
    "        bar_input = input(\"Columns for bar charts (comma-separated) or 'none': \").strip()\n",
    "        if bar_input.lower() != 'none':\n",
    "            bar_cols = [c.strip() for c in bar_input.split(\",\") if c.strip() in df_columns]\n",
    "            if not bar_cols:\n",
    "                print(\"‚ö†Ô∏è No valid columns selected for bar charts.\")\n",
    "\n",
    "        pie_input = input(\"Columns for pie charts (comma-separated) or 'none': \").strip()\n",
    "        if pie_input.lower() != 'none':\n",
    "            pie_cols = [c.strip() for c in pie_input.split(\",\") if c.strip() in df_columns]\n",
    "            if not pie_cols:\n",
    "                print(\"‚ö†Ô∏è No valid columns selected for pie charts.\")\n",
    "\n",
    "    # Sampling prompt\n",
    "    sample_size = None\n",
    "    default_sample_size = 50000 if row_count > 150000 else None\n",
    "    while True:\n",
    "        sample_prompt = f\"Enter sample size for visualizations (e.g., 50000) [default: {default_sample_size if default_sample_size else 'full dataset'}]: \"\n",
    "        sample_input = input(sample_prompt).strip().lower()\n",
    "        if sample_input == '' and default_sample_size is not None:\n",
    "            sample_size = default_sample_size\n",
    "            break\n",
    "        if sample_input == 'none':\n",
    "            break\n",
    "        try:\n",
    "            sample_size = int(sample_input)\n",
    "            if sample_size > 0:\n",
    "                break\n",
    "            print(\"‚ö†Ô∏è Sample size must be positive.\")\n",
    "        except ValueError:\n",
    "            print(\"‚ö†Ô∏è Invalid input. Enter a number or 'none'.\")\n",
    "\n",
    "    return {\n",
    "        'missing_values': {'1': 'drop', '2': 'fill', '3': 'none'}[mv_choice],\n",
    "        'drop_duplicates': dd_choice == 'y',\n",
    "        'bar_columns': bar_cols,\n",
    "        'pie_columns': pie_cols,\n",
    "        'sample_size': sample_size\n",
    "    }\n",
    "\n",
    "def clean_data(df: pd.DataFrame, choices: dict) -> tuple[pd.DataFrame, dict]:\n",
    "    \"\"\"Cleans the DataFrame based on user-specified rules.\"\"\"\n",
    "    print(\"‚è≥ Cleaning data...\")\n",
    "    logging.info(\"Starting data cleaning\")\n",
    "    df_cleaned = df.copy()\n",
    "    original_cols = df_cleaned.columns.tolist()\n",
    "\n",
    "    if choices['drop_duplicates']:\n",
    "        before = len(df_cleaned)\n",
    "        df_cleaned.drop_duplicates(inplace=True)\n",
    "        print(f\"  - Dropped {before - len(df_cleaned)} duplicate rows.\")\n",
    "        logging.info(f\"Dropped {before - len(df_cleaned)} duplicate rows\")\n",
    "\n",
    "    if choices['missing_values'] == 'drop':\n",
    "        before = len(df_cleaned)\n",
    "        df_cleaned.dropna(inplace=True)\n",
    "        print(f\"  - Dropped {before - len(df_cleaned)} rows with NA.\")\n",
    "        logging.info(f\"Dropped {before - len(df_cleaned)} rows with NA\")\n",
    "    elif choices['missing_values'] == 'fill':\n",
    "        for col in df_cleaned.columns:\n",
    "            if df_cleaned[col].isnull().any():\n",
    "                if pd.api.types.is_numeric_dtype(df_cleaned[col]):\n",
    "                    df_cleaned[col].fillna(df_cleaned[col].mean(), inplace=True)\n",
    "                else:\n",
    "                    df_cleaned[col].fillna(df_cleaned[col].mode().iloc[0] if not df_cleaned[col].mode().empty else \"Unknown\", inplace=True)\n",
    "        print(\"  - Filled missing values with mean/mode.\")\n",
    "        logging.info(\"Filled missing values with mean/mode\")\n",
    "\n",
    "    cleaned_cols = [c.strip().replace(\" \", \"_\") for c in df_cleaned.columns]\n",
    "    mapping = dict(zip(original_cols, cleaned_cols))\n",
    "    df_cleaned.columns = cleaned_cols\n",
    "\n",
    "    df_cleaned = df_cleaned.apply(lambda s: pd.to_numeric(s, errors='ignore'))\n",
    "\n",
    "    print(f\"‚úÖ Cleaning complete. Shape: {df_cleaned.shape}\")\n",
    "    logging.info(f\"Cleaning complete. Shape: {df_cleaned.shape}\")\n",
    "    return df_cleaned, mapping\n",
    "\n",
    "def generate_visualizations(df: pd.DataFrame, base_filename: str, output_folder: str, choices: dict):\n",
    "    \"\"\"Generates bar and pie chart visualizations.\"\"\"\n",
    "    print(\"üìä Creating visualizations...\")\n",
    "    logging.info(f\"Creating visualizations for {base_filename}\")\n",
    "    plot_folder = os.path.join(output_folder, f\"{base_filename}_plots\")\n",
    "    os.makedirs(plot_folder, exist_ok=True)\n",
    "\n",
    "    # Apply sampling if specified\n",
    "    df_plot = df\n",
    "    if choices['sample_size'] is not None and choices['sample_size'] < len(df):\n",
    "        df_plot = df.sample(n=choices['sample_size'], random_state=42)\n",
    "        print(f\"  - Using sample of {choices['sample_size']} rows for visualizations\")\n",
    "        logging.info(f\"Using sample of {choices['sample_size']} rows for visualizations\")\n",
    "\n",
    "    categorical_cols = df_plot.select_dtypes(include=['object', 'category']).columns\n",
    "    filtered_cols = [c for c in categorical_cols if not any(pat in c.lower() for pat in EXCLUDE_PATTERNS)]\n",
    "\n",
    "    bar_columns = choices['bar_columns']\n",
    "    pie_columns = choices['pie_columns']\n",
    "    if not (bar_columns or pie_columns):\n",
    "        pie_columns = [c for c in filtered_cols if 2 <= df_plot[c].nunique() <= 10][:2]\n",
    "        bar_columns = [c for c in filtered_cols if 5 <= df_plot[c].nunique() <= 100][:2]\n",
    "\n",
    "    for col in bar_columns:\n",
    "        if col in df_plot:\n",
    "            plt.figure(figsize=VISUALIZATION_OPTIONS['figure_size'])\n",
    "            top10 = df_plot[col].value_counts().nlargest(10)\n",
    "            sns.barplot(x=top10.values, y=top10.index, palette=VISUALIZATION_OPTIONS['colors'][:len(top10)])\n",
    "            plt.title(f\"Top 10 {col}\")\n",
    "            plt.savefig(os.path.join(plot_folder, f\"bar_{col}.png\"))\n",
    "            plt.close()\n",
    "            print(f\"  - Bar chart: {col}\")\n",
    "            logging.info(f\"Generated bar chart for {col}\")\n",
    "\n",
    "    for col in pie_columns:\n",
    "        if col in df_plot and 2 <= df_plot[col].nunique() <= 10:\n",
    "            plt.figure(figsize=VISUALIZATION_OPTIONS['figure_size'])\n",
    "            counts = df_plot[col].value_counts()\n",
    "            plt.pie(counts, labels=counts.index, autopct='%1.1f%%', colors=VISUALIZATION_OPTIONS['colors'][:len(counts)])\n",
    "            plt.title(f\"Distribution of {col}\")\n",
    "            plt.savefig(os.path.join(plot_folder, f\"pie_{col}.png\"))\n",
    "            plt.close()\n",
    "            print(f\"  - Pie chart: {col}\")\n",
    "            logging.info(f\"Generated pie chart for {col}\")\n",
    "\n",
    "    if not (bar_columns or pie_columns):\n",
    "        print(\"‚ö†Ô∏è No suitable columns for visualization.\")\n",
    "        logging.warning(\"No suitable columns for visualization\")\n",
    "\n",
    "    print(f\"‚úÖ Plots saved in {plot_folder}\")\n",
    "    logging.info(f\"Plots saved in {plot_folder}\")\n",
    "\n",
    "def generate_ydata_report(df: pd.DataFrame, base_filename: str, output_folder: str):\n",
    "    \"\"\"Generates an interactive HTML profiling report in minimal mode for large datasets.\"\"\"\n",
    "    print(\"üìë Building profiling report...\")\n",
    "    logging.info(f\"Building profiling report for {base_filename}\")\n",
    "    try:\n",
    "        with redirect_stdout(StringIO()):\n",
    "            profile = ProfileReport(\n",
    "                df,\n",
    "                title=f\"Profiling Report for {base_filename}\",\n",
    "                minimal=len(df) > 10000,\n",
    "                explorative=True,\n",
    "                progress_bar=False  # Disable progress bar and widget\n",
    "            )\n",
    "            outpath = os.path.join(output_folder, f\"{base_filename}_report_{datetime.now().strftime('%Y%m%d')}.html\")\n",
    "            profile.to_file(outpath, silent=True)\n",
    "        print(f\"‚úÖ Report saved -> {outpath}\")\n",
    "        logging.info(f\"Report saved: {outpath}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Report generation failed for {base_filename}: {e}\")\n",
    "        print(f\"‚ùå Report generation failed: {e}\")\n",
    "\n",
    "def save_outputs(df_cleaned: pd.DataFrame, base_filename: str, output_folder: str):\n",
    "    \"\"\"Saves the cleaned DataFrame to CSV and Excel (if within row limit).\"\"\"\n",
    "    print(\"üíæ Saving cleaned dataset...\")\n",
    "    logging.info(f\"Saving cleaned dataset for {base_filename}\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "    csv_path = os.path.join(output_folder, f\"{base_filename}_cleaned_{timestamp}.csv\")\n",
    "    df_cleaned.to_csv(csv_path, index=False)\n",
    "    print(f\"‚úÖ Saved CSV -> {csv_path}\")\n",
    "    logging.info(f\"Saved CSV: {csv_path}\")\n",
    "\n",
    "    if df_cleaned.shape[0] <= 1_048_576:\n",
    "        try:\n",
    "            xlsx_path = os.path.join(output_folder, f\"{base_filename}_cleaned_{timestamp}.xlsx\")\n",
    "            df_cleaned.to_excel(xlsx_path, index=False)\n",
    "            print(f\"‚úÖ Saved Excel -> {xlsx_path}\")\n",
    "            logging.info(f\"Saved Excel: {xlsx_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to save Excel for {base_filename}: {e}\")\n",
    "            print(f\"‚ùå Failed to save Excel: {e}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Skipped Excel export (too many rows for Excel).\")\n",
    "        logging.warning(f\"Skipped Excel export for {base_filename}: too many rows\")\n",
    "\n",
    "def process_file(file_path: str):\n",
    "    \"\"\"Processes a single file (used for parallel processing).\"\"\"\n",
    "    base = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    print(f\"\\n{'='*20} Processing {base} {'='*20}\")\n",
    "    logging.info(f\"Processing file: {base}\")\n",
    "\n",
    "    raw_df = load_data(file_path)\n",
    "    if raw_df is None:\n",
    "        return\n",
    "\n",
    "    cleaned_df, _ = clean_data(raw_df, {'missing_values': 'none', 'drop_duplicates': False})\n",
    "    cleaning_choices = get_cleaning_choices(cleaned_df.columns, len(cleaned_df))\n",
    "    cleaned_df, _ = clean_data(cleaned_df, cleaning_choices)\n",
    "    save_outputs(cleaned_df, base, OUTPUT_FOLDER)\n",
    "    generate_visualizations(cleaned_df, base, OUTPUT_FOLDER, cleaning_choices)\n",
    "    generate_ydata_report(raw_df, base, OUTPUT_FOLDER)\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    logging.info(\"Starting data processing script\")\n",
    "    selected_files = list_and_select_files(DATA_FOLDER)\n",
    "    if not selected_files:\n",
    "        logging.info(\"No files selected. Exiting.\")\n",
    "        print(\"No files selected. Exiting.\")\n",
    "        return\n",
    "\n",
    "    for file_path in selected_files:\n",
    "        process_file(file_path)\n",
    "\n",
    "    print(\"\\nüéâ Automation complete!\")\n",
    "    logging.info(\"Automation complete\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2050e016-4a6a-4fe5-9300-e40a5e0a2f09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
