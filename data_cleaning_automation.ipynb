{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c0ed5d-a9da-4416-94d4-b383b8c8fc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from ydata_profiling import ProfileReport\n",
    "import warnings\n",
    "from io import StringIO\n",
    "from contextlib import redirect_stdout\n",
    "import logging\n",
    "from multiprocessing import Pool\n",
    "import tqdm  # Import tqdm to disable progress bars\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Disable tqdm progress bars globally\n",
    "tqdm.tqdm().disable = True\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIGURATION\n",
    "# -----------------------------\n",
    "\n",
    "# Dynamic data folder input with validation\n",
    "def get_valid_directory():\n",
    "    \"\"\"Prompts for and validates a directory path.\"\"\"\n",
    "    while True:\n",
    "        data_folder = input(\"Enter dataset folder path (e.g., C:\\\\Users\\\\Admin\\\\Documents\\\\Datasets) [default: current directory]: \").strip() or os.getcwd()\n",
    "        if os.path.isfile(data_folder):\n",
    "            logging.warning(f\"Provided path is a file, not a directory: {data_folder}\")\n",
    "            print(f\"⚠️ Error: '{data_folder}' is a file, not a directory. Using parent directory.\")\n",
    "            data_folder = os.path.dirname(data_folder)\n",
    "        if os.path.isdir(data_folder):\n",
    "            return data_folder\n",
    "        else:\n",
    "            logging.error(f\"Invalid directory: {data_folder}\")\n",
    "            print(f\"❌ Error: '{data_folder}' is not a valid directory. Please try again.\")\n",
    "\n",
    "DATA_FOLDER = get_valid_directory()\n",
    "OUTPUT_FOLDER = \"output\"\n",
    "SHOW_PREVIEW = True\n",
    "EXCLUDE_PATTERNS = ['id', 'desc', 'number', 'phone', 'contact', 'name']\n",
    "VISUALIZATION_OPTIONS = {\n",
    "    'figure_size': (10, 6),  # Hardcoded default figure size\n",
    "    'colors': ['#FF6384', '#36A2EB', '#FFCE56', '#4BC0C0', '#9966FF']\n",
    "}\n",
    "\n",
    "# Setup logging\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    filename=os.path.join(OUTPUT_FOLDER, f'processing_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'),\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def list_and_select_files(data_folder: str) -> list:\n",
    "    \"\"\"Lists available datasets and lets user pick one or more for processing.\"\"\"\n",
    "    try:\n",
    "        files = [f for f in os.listdir(data_folder) if f.lower().endswith((\".xlsx\", \".xls\", \".csv\", \".json\", \".parquet\"))]\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"The specified data folder does not exist: {data_folder}\")\n",
    "        print(f\"❌ ERROR: The specified data folder does not exist: {data_folder}\")\n",
    "        return []\n",
    "    except OSError as e:\n",
    "        logging.error(f\"Error accessing directory {data_folder}: {e}\")\n",
    "        print(f\"❌ ERROR: Unable to access directory {data_folder}: {e}\")\n",
    "        return []\n",
    "\n",
    "    if not files:\n",
    "        logging.warning(f\"No datasets found in: {data_folder}\")\n",
    "        print(f\"❌ No datasets found in: {data_folder}\")\n",
    "        return []\n",
    "\n",
    "    print(\"\\nAvailable datasets:\")\n",
    "    for i, f in enumerate(files, 1):\n",
    "        print(f\"  {i}. {f}\")\n",
    "\n",
    "    choice_str = input(\"\\nEnter number(s) of dataset(s) to process, comma-separated (e.g., 1,3) [default: last]: \").strip()\n",
    "\n",
    "    selected_files = []\n",
    "    if not choice_str:\n",
    "        selected_files.append(os.path.join(data_folder, files[-1]))  # Default to last file\n",
    "    else:\n",
    "        try:\n",
    "            choices = [int(c.strip()) - 1 for c in choice_str.split(\",\")]\n",
    "            selected_files = [os.path.join(data_folder, files[i]) for i in choices if 0 <= i < len(files)]\n",
    "            if not selected_files:\n",
    "                logging.warning(\"No valid file indices selected. Defaulting to the last dataset.\")\n",
    "                print(\"⚠️ No valid file indices selected. Defaulting to the last dataset.\")\n",
    "                selected_files.append(os.path.join(data_folder, files[-1]))\n",
    "        except (ValueError, IndexError):\n",
    "            logging.error(f\"Invalid input: {choice_str}. Defaulting to the last dataset.\")\n",
    "            print(\"⚠️ Invalid input. Defaulting to the last dataset.\")\n",
    "            selected_files.append(os.path.join(data_folder, files[-1]))\n",
    "\n",
    "    return selected_files\n",
    "\n",
    "def load_data(file_path: str) -> pd.DataFrame | None:\n",
    "    \"\"\"Loads dataset (CSV, Excel, JSON, Parquet) into a pandas DataFrame.\"\"\"\n",
    "    print(f\"\\n⏳ Loading data from: {os.path.basename(file_path)}\")\n",
    "    logging.info(f\"Loading data from: {file_path}\")\n",
    "    try:\n",
    "        if file_path.lower().endswith(\".csv\"):\n",
    "            encodings = ['utf-8', 'latin1', 'iso-8859-1']\n",
    "            for enc in encodings:\n",
    "                try:\n",
    "                    df = pd.read_csv(file_path, encoding=enc)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Tried encoding {enc} for {file_path}: {e}\")\n",
    "            else:\n",
    "                raise ValueError(\"Failed to load CSV with any encoding\")\n",
    "        elif file_path.lower().endswith((\".xlsx\", \".xls\")):\n",
    "            df = pd.read_excel(file_path)\n",
    "        elif file_path.lower().endswith(\".json\"):\n",
    "            df = pd.read_json(file_path)\n",
    "        elif file_path.lower().endswith(\".parquet\"):\n",
    "            df = pd.read_parquet(file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format\")\n",
    "        \n",
    "        print(f\"✅ Loaded successfully with shape {df.shape}\")\n",
    "        logging.info(f\"Loaded successfully: {file_path}, shape {df.shape}\")\n",
    "        if SHOW_PREVIEW:\n",
    "            print(df.head())\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading {os.path.basename(file_path)}: {e}\")\n",
    "        print(f\"❌ Error loading {os.path.basename(file_path)}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_cleaning_choices(df_columns: list, row_count: int) -> dict:\n",
    "    \"\"\"Asks the user for cleaning, visualization, and sampling preferences with validation.\"\"\"\n",
    "    print(\"\\n--- Data Cleaning Options ---\")\n",
    "    \n",
    "    while True:\n",
    "        mv_choice = input(\"Handle missing values?\\n [1] Drop rows\\n [2] Fill mean/mode\\n [3] Do nothing\\n Choice: \").strip()\n",
    "        if mv_choice in ['1', '2', '3']:\n",
    "            break\n",
    "        print(\"⚠️ Invalid choice. Please select 1, 2, or 3.\")\n",
    "\n",
    "    while True:\n",
    "        dd_choice = input(\"Drop duplicate rows? (y/n): \").strip().lower()\n",
    "        if dd_choice in ['y', 'n']:\n",
    "            break\n",
    "        print(\"⚠️ Invalid choice. Please select y or n.\")\n",
    "\n",
    "    bar_cols, pie_cols = [], []\n",
    "    print(\"\\n--- Visualization Options ---\")\n",
    "    print(f\"Available columns: {df_columns}\")\n",
    "    if input(\"Manually select columns for visualizations? (y/n): \").strip().lower() == 'y':\n",
    "        bar_input = input(\"Columns for bar charts (comma-separated) or 'none': \").strip()\n",
    "        if bar_input.lower() != 'none':\n",
    "            bar_cols = [c.strip() for c in bar_input.split(\",\") if c.strip() in df_columns]\n",
    "            if not bar_cols:\n",
    "                print(\"⚠️ No valid columns selected for bar charts.\")\n",
    "\n",
    "        pie_input = input(\"Columns for pie charts (comma-separated) or 'none': \").strip()\n",
    "        if pie_input.lower() != 'none':\n",
    "            pie_cols = [c.strip() for c in pie_input.split(\",\") if c.strip() in df_columns]\n",
    "            if not pie_cols:\n",
    "                print(\"⚠️ No valid columns selected for pie charts.\")\n",
    "\n",
    "    # Sampling prompt\n",
    "    sample_size = None\n",
    "    default_sample_size = 50000 if row_count > 150000 else None\n",
    "    while True:\n",
    "        sample_prompt = f\"Enter sample size for visualizations (e.g., 50000) [default: {default_sample_size if default_sample_size else 'full dataset'}]: \"\n",
    "        sample_input = input(sample_prompt).strip().lower()\n",
    "        if sample_input == '' and default_sample_size is not None:\n",
    "            sample_size = default_sample_size\n",
    "            break\n",
    "        if sample_input == 'none':\n",
    "            break\n",
    "        try:\n",
    "            sample_size = int(sample_input)\n",
    "            if sample_size > 0:\n",
    "                break\n",
    "            print(\"⚠️ Sample size must be positive.\")\n",
    "        except ValueError:\n",
    "            print(\"⚠️ Invalid input. Enter a number or 'none'.\")\n",
    "\n",
    "    return {\n",
    "        'missing_values': {'1': 'drop', '2': 'fill', '3': 'none'}[mv_choice],\n",
    "        'drop_duplicates': dd_choice == 'y',\n",
    "        'bar_columns': bar_cols,\n",
    "        'pie_columns': pie_cols,\n",
    "        'sample_size': sample_size\n",
    "    }\n",
    "\n",
    "def clean_data(df: pd.DataFrame, choices: dict) -> tuple[pd.DataFrame, dict]:\n",
    "    \"\"\"Cleans the DataFrame based on user-specified rules.\"\"\"\n",
    "    print(\"⏳ Cleaning data...\")\n",
    "    logging.info(\"Starting data cleaning\")\n",
    "    df_cleaned = df.copy()\n",
    "    original_cols = df_cleaned.columns.tolist()\n",
    "\n",
    "    if choices['drop_duplicates']:\n",
    "        before = len(df_cleaned)\n",
    "        df_cleaned.drop_duplicates(inplace=True)\n",
    "        print(f\"  - Dropped {before - len(df_cleaned)} duplicate rows.\")\n",
    "        logging.info(f\"Dropped {before - len(df_cleaned)} duplicate rows\")\n",
    "\n",
    "    if choices['missing_values'] == 'drop':\n",
    "        before = len(df_cleaned)\n",
    "        df_cleaned.dropna(inplace=True)\n",
    "        print(f\"  - Dropped {before - len(df_cleaned)} rows with NA.\")\n",
    "        logging.info(f\"Dropped {before - len(df_cleaned)} rows with NA\")\n",
    "    elif choices['missing_values'] == 'fill':\n",
    "        for col in df_cleaned.columns:\n",
    "            if df_cleaned[col].isnull().any():\n",
    "                if pd.api.types.is_numeric_dtype(df_cleaned[col]):\n",
    "                    df_cleaned[col].fillna(df_cleaned[col].mean(), inplace=True)\n",
    "                else:\n",
    "                    df_cleaned[col].fillna(df_cleaned[col].mode().iloc[0] if not df_cleaned[col].mode().empty else \"Unknown\", inplace=True)\n",
    "        print(\"  - Filled missing values with mean/mode.\")\n",
    "        logging.info(\"Filled missing values with mean/mode\")\n",
    "\n",
    "    cleaned_cols = [c.strip().replace(\" \", \"_\") for c in df_cleaned.columns]\n",
    "    mapping = dict(zip(original_cols, cleaned_cols))\n",
    "    df_cleaned.columns = cleaned_cols\n",
    "\n",
    "    df_cleaned = df_cleaned.apply(lambda s: pd.to_numeric(s, errors='ignore'))\n",
    "\n",
    "    print(f\"✅ Cleaning complete. Shape: {df_cleaned.shape}\")\n",
    "    logging.info(f\"Cleaning complete. Shape: {df_cleaned.shape}\")\n",
    "    return df_cleaned, mapping\n",
    "\n",
    "def generate_visualizations(df: pd.DataFrame, base_filename: str, output_folder: str, choices: dict):\n",
    "    \"\"\"Generates bar and pie chart visualizations.\"\"\"\n",
    "    print(\"📊 Creating visualizations...\")\n",
    "    logging.info(f\"Creating visualizations for {base_filename}\")\n",
    "    plot_folder = os.path.join(output_folder, f\"{base_filename}_plots\")\n",
    "    os.makedirs(plot_folder, exist_ok=True)\n",
    "\n",
    "    # Apply sampling if specified\n",
    "    df_plot = df\n",
    "    if choices['sample_size'] is not None and choices['sample_size'] < len(df):\n",
    "        df_plot = df.sample(n=choices['sample_size'], random_state=42)\n",
    "        print(f\"  - Using sample of {choices['sample_size']} rows for visualizations\")\n",
    "        logging.info(f\"Using sample of {choices['sample_size']} rows for visualizations\")\n",
    "\n",
    "    categorical_cols = df_plot.select_dtypes(include=['object', 'category']).columns\n",
    "    filtered_cols = [c for c in categorical_cols if not any(pat in c.lower() for pat in EXCLUDE_PATTERNS)]\n",
    "\n",
    "    bar_columns = choices['bar_columns']\n",
    "    pie_columns = choices['pie_columns']\n",
    "    if not (bar_columns or pie_columns):\n",
    "        pie_columns = [c for c in filtered_cols if 2 <= df_plot[c].nunique() <= 10][:2]\n",
    "        bar_columns = [c for c in filtered_cols if 5 <= df_plot[c].nunique() <= 100][:2]\n",
    "\n",
    "    for col in bar_columns:\n",
    "        if col in df_plot:\n",
    "            plt.figure(figsize=VISUALIZATION_OPTIONS['figure_size'])\n",
    "            top10 = df_plot[col].value_counts().nlargest(10)\n",
    "            sns.barplot(x=top10.values, y=top10.index, palette=VISUALIZATION_OPTIONS['colors'][:len(top10)])\n",
    "            plt.title(f\"Top 10 {col}\")\n",
    "            plt.savefig(os.path.join(plot_folder, f\"bar_{col}.png\"))\n",
    "            plt.close()\n",
    "            print(f\"  - Bar chart: {col}\")\n",
    "            logging.info(f\"Generated bar chart for {col}\")\n",
    "\n",
    "    for col in pie_columns:\n",
    "        if col in df_plot and 2 <= df_plot[col].nunique() <= 10:\n",
    "            plt.figure(figsize=VISUALIZATION_OPTIONS['figure_size'])\n",
    "            counts = df_plot[col].value_counts()\n",
    "            plt.pie(counts, labels=counts.index, autopct='%1.1f%%', colors=VISUALIZATION_OPTIONS['colors'][:len(counts)])\n",
    "            plt.title(f\"Distribution of {col}\")\n",
    "            plt.savefig(os.path.join(plot_folder, f\"pie_{col}.png\"))\n",
    "            plt.close()\n",
    "            print(f\"  - Pie chart: {col}\")\n",
    "            logging.info(f\"Generated pie chart for {col}\")\n",
    "\n",
    "    if not (bar_columns or pie_columns):\n",
    "        print(\"⚠️ No suitable columns for visualization.\")\n",
    "        logging.warning(\"No suitable columns for visualization\")\n",
    "\n",
    "    print(f\"✅ Plots saved in {plot_folder}\")\n",
    "    logging.info(f\"Plots saved in {plot_folder}\")\n",
    "\n",
    "def generate_ydata_report(df: pd.DataFrame, base_filename: str, output_folder: str):\n",
    "    \"\"\"Generates an interactive HTML profiling report in minimal mode for large datasets.\"\"\"\n",
    "    print(\"📑 Building profiling report...\")\n",
    "    logging.info(f\"Building profiling report for {base_filename}\")\n",
    "    try:\n",
    "        with redirect_stdout(StringIO()):\n",
    "            profile = ProfileReport(\n",
    "                df,\n",
    "                title=f\"Profiling Report for {base_filename}\",\n",
    "                minimal=len(df) > 10000,\n",
    "                explorative=True,\n",
    "                progress_bar=False  # Disable progress bar and widget\n",
    "            )\n",
    "            outpath = os.path.join(output_folder, f\"{base_filename}_report_{datetime.now().strftime('%Y%m%d')}.html\")\n",
    "            profile.to_file(outpath, silent=True)\n",
    "        print(f\"✅ Report saved -> {outpath}\")\n",
    "        logging.info(f\"Report saved: {outpath}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Report generation failed for {base_filename}: {e}\")\n",
    "        print(f\"❌ Report generation failed: {e}\")\n",
    "\n",
    "def save_outputs(df_cleaned: pd.DataFrame, base_filename: str, output_folder: str):\n",
    "    \"\"\"Saves the cleaned DataFrame to CSV and Excel (if within row limit).\"\"\"\n",
    "    print(\"💾 Saving cleaned dataset...\")\n",
    "    logging.info(f\"Saving cleaned dataset for {base_filename}\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "    csv_path = os.path.join(output_folder, f\"{base_filename}_cleaned_{timestamp}.csv\")\n",
    "    df_cleaned.to_csv(csv_path, index=False)\n",
    "    print(f\"✅ Saved CSV -> {csv_path}\")\n",
    "    logging.info(f\"Saved CSV: {csv_path}\")\n",
    "\n",
    "    if df_cleaned.shape[0] <= 1_048_576:\n",
    "        try:\n",
    "            xlsx_path = os.path.join(output_folder, f\"{base_filename}_cleaned_{timestamp}.xlsx\")\n",
    "            df_cleaned.to_excel(xlsx_path, index=False)\n",
    "            print(f\"✅ Saved Excel -> {xlsx_path}\")\n",
    "            logging.info(f\"Saved Excel: {xlsx_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to save Excel for {base_filename}: {e}\")\n",
    "            print(f\"❌ Failed to save Excel: {e}\")\n",
    "    else:\n",
    "        print(\"⚠️ Skipped Excel export (too many rows for Excel).\")\n",
    "        logging.warning(f\"Skipped Excel export for {base_filename}: too many rows\")\n",
    "\n",
    "def process_file(file_path: str):\n",
    "    \"\"\"Processes a single file (used for parallel processing).\"\"\"\n",
    "    base = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    print(f\"\\n{'='*20} Processing {base} {'='*20}\")\n",
    "    logging.info(f\"Processing file: {base}\")\n",
    "\n",
    "    raw_df = load_data(file_path)\n",
    "    if raw_df is None:\n",
    "        return\n",
    "\n",
    "    cleaned_df, _ = clean_data(raw_df, {'missing_values': 'none', 'drop_duplicates': False})\n",
    "    cleaning_choices = get_cleaning_choices(cleaned_df.columns, len(cleaned_df))\n",
    "    cleaned_df, _ = clean_data(cleaned_df, cleaning_choices)\n",
    "    save_outputs(cleaned_df, base, OUTPUT_FOLDER)\n",
    "    generate_visualizations(cleaned_df, base, OUTPUT_FOLDER, cleaning_choices)\n",
    "    generate_ydata_report(raw_df, base, OUTPUT_FOLDER)\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    logging.info(\"Starting data processing script\")\n",
    "    selected_files = list_and_select_files(DATA_FOLDER)\n",
    "    if not selected_files:\n",
    "        logging.info(\"No files selected. Exiting.\")\n",
    "        print(\"No files selected. Exiting.\")\n",
    "        return\n",
    "\n",
    "    for file_path in selected_files:\n",
    "        process_file(file_path)\n",
    "\n",
    "    print(\"\\n🎉 Automation complete!\")\n",
    "    logging.info(\"Automation complete\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
